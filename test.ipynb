{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from megatron.data.indexed_dataset import MMapIndexedDataset\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class GPT2Dataset(Dataset):\n",
    "    \"\"\"Streamlined version of the GPT2Dataset in megatron.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        indexed_dataset: MMapIndexedDataset,\n",
    "        doc_idx: np.memmap,\n",
    "        sample_idx: np.memmap,\n",
    "        shuffle_idx: np.memmap,\n",
    "    ):\n",
    "        self.indexed_dataset = indexed_dataset\n",
    "        self.doc_idx = doc_idx\n",
    "        self.sample_idx = sample_idx\n",
    "        self.shuffle_idx = shuffle_idx\n",
    "\n",
    "        self.shuffle_idx_len = self.shuffle_idx.shape[0] - 1\n",
    "        self.sample_idx_len = self.sample_idx.shape[0] - 1\n",
    "\n",
    "        if self.shuffle_idx_len != self.sample_idx_len:\n",
    "            print(f\"WARNING: {self.shuffle_idx_len=} != {self.sample_idx_len=}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(self.shuffle_idx_len, self.sample_idx_len)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            # Get the shuffled index.\n",
    "            idx = self.shuffle_idx[idx]\n",
    "            # Start and end documents and offsets.\n",
    "            doc_index_f = self.sample_idx[idx][0]\n",
    "            doc_index_l = self.sample_idx[idx + 1][0]\n",
    "            offset_f = self.sample_idx[idx][1]\n",
    "            offset_l = self.sample_idx[idx + 1][1]\n",
    "            # If we are within the same document, just extract the chunk.\n",
    "            if doc_index_f == doc_index_l:\n",
    "                sample = self.indexed_dataset.get(\n",
    "                    self.doc_idx[doc_index_f],\n",
    "                    offset=offset_f,\n",
    "                    length=offset_l - offset_f + 1,\n",
    "                )\n",
    "            else:\n",
    "                # Otherwise, get the rest of the initial document.\n",
    "                sample_list = [\n",
    "                    self.indexed_dataset.get(self.doc_idx[doc_index_f], offset=offset_f)\n",
    "                ]\n",
    "                # Loop over all in between documents and add the entire document.\n",
    "                for i in range(doc_index_f + 1, doc_index_l):\n",
    "                    sample_list.append(self.indexed_dataset.get(self.doc_idx[i]))\n",
    "                # And finally add the relevant portion of last document.\n",
    "                sample_list.append(\n",
    "                    self.indexed_dataset.get(\n",
    "                        self.doc_idx[doc_index_l], length=offset_l + 1\n",
    "                    )\n",
    "                )\n",
    "                sample = np.concatenate(sample_list)\n",
    "\n",
    "            return {\"text\": np.array(sample, dtype=np.int64)}\n",
    "        except IndexError:\n",
    "            new_idx = idx % len(self)\n",
    "            print(\n",
    "                f\"WARNING: Got index out of bounds error with index {idx} - taking modulo of index instead ({new_idx})\"\n",
    "            )\n",
    "            return self[new_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(file_path: Path, pattern: str) -> GPT2Dataset:\n",
    "    # e.g., pile_20B_tokenizer_text_document_train_indexmap_120ns_2048sl_1234s_doc_idx.npy\n",
    "    # pattern: pile_20B_tokenizer_text_document_train_indexmap_120ns_2048sl_1234s\n",
    "    \n",
    "    doc_idx = np.load(file_path / f\"{pattern}_doc_idx.npy\", allow_pickle=True, mmap_mode=\"r\")\n",
    "    sample_idx = np.load(file_path / f\"{pattern}_sample_idx.npy\", allow_pickle=True, mmap_mode=\"r\")\n",
    "    shuffle_idx = np.load(file_path / f\"{pattern}_shuffle_idx.npy\", allow_pickle=True, mmap_mode=\"r\")\n",
    "    indexed_dataset = MMapIndexedDataset(str(file_path / \"pile_20B_tokenizer_text_document\"), skip_warmup=True)\n",
    "\n",
    "    ds = GPT2Dataset(\n",
    "        indexed_dataset=indexed_dataset,\n",
    "        doc_idx=doc_idx,\n",
    "        sample_idx=sample_idx,\n",
    "        shuffle_idx=shuffle_idx,\n",
    "    )\n",
    "\n",
    "    # check seqlen is correct\n",
    "    print(f\"Seq length ==\", len(ds[0][\"text\"]))\n",
    "    print(f\"Num batches ==\", len(ds) / 1024, \"(should be 143k)\")\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      "WARNING: self.shuffle_idx_len=157147139 != self.sample_idx_len=157147140\n",
      "Seq length == 2049\n",
      "Num batches == 153464.0029296875 (should be 143k)\n"
     ]
    }
   ],
   "source": [
    "file_path = Path(\"/mnt/ssd-2/pile_deduped/\")\n",
    "pattern = \"pile_20B_tokenizer_text_document_train_indexmap_120ns_2048sl_1234s\"\n",
    "ds = read_dataset(file_path, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      "WARNING: self.shuffle_idx_len=157147139 != self.sample_idx_len=157147140\n",
      "Seq length == 2049\n",
      "Num batches == 153464.0029296875 (should be 143k)\n"
     ]
    }
   ],
   "source": [
    "file_path = Path(\"/mnt/ssd-2/pile_deduped/\")\n",
    "pattern = \"pile_20B_tokenizer_text_document_train_indexmap_960ns_2048sl_1234s\"\n",
    "_ = read_dataset(file_path, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      "WARNING: self.shuffle_idx_len=162165685 != self.sample_idx_len=162165686\n",
      "Seq length == 2049\n",
      "Num batches == 158364.9267578125 (should be 143k)\n"
     ]
    }
   ],
   "source": [
    "file_path = Path(\"/mnt/ssd-2/pile_extra_seeds/\")\n",
    "pattern = \"pile_20B_tokenizer_text_document_train_0_indexmap_258ns_2048sl_1s\"\n",
    "oskar_ds = read_dataset(file_path, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oskar_ds.indexed_dataset) == len(ds.indexed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oskar_ds.sample_idx) == len(ds.sample_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oskar_ds.indexed_dataset.sizes.shape[0] == ds.indexed_dataset.sizes.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
